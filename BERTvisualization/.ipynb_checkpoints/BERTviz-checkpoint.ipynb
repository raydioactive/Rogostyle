{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa9e718a-5da1-4ef4-9496-892916fa158f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-28 12:06:05.550412: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\n",
      "2024-03-28 12:06:05.550457: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c9543b-6109-4ffc-a4a7-68ae7d6d297e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = \"/slipstream_old/home/juliusherzog/latinbert/models/subword_tokenizer_latin/latin.subword.encoder\"\n",
    "                    \n",
    "# Load pre-trained model (weights)\n",
    "model = \"/slipstream_old/home/juliusherzog/latinbert/models/latin_bert/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f17488-2800-42f9-92f6-cd828090dfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, sys\n",
    "from cltk.tokenizers.lat.lat import LatinWordTokenizer as WordTokenizer\n",
    "from cltk.tokenizers.lat.lat import LatinPunktSentenceTokenizer as SentenceTokenizer\n",
    "from tensor2tensor.data_generators import text_encoder\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertModel, BertPreTrainedModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class LatinBERT():\n",
    "\n",
    "\tdef __init__(self, tokenizerPath=None, bertPath=None):\n",
    "\t\tencoder = text_encoder.SubwordTextEncoder(tokenizerPath)\n",
    "\t\tself.wp_tokenizer = LatinTokenizer(encoder)\n",
    "\t\tself.model = BertLatin(bertPath=bertPath)\n",
    "\t\tself.model.to(device)\n",
    "\n",
    "\tdef get_batches(self, sentences, max_batch, tokenizer):\n",
    "\n",
    "\t\t\tmaxLen=0\n",
    "\t\t\tfor sentence in sentences:\n",
    "\t\t\t\tlength=0\n",
    "\t\t\t\tfor word in sentence:\n",
    "\t\t\t\t\ttoks=tokenizer.tokenize(word)\n",
    "\t\t\t\t\tlength+=len(toks)\n",
    "\n",
    "\t\t\t\tif length> maxLen:\n",
    "\t\t\t\t\tmaxLen=length\n",
    "\n",
    "\t\t\tall_data=[]\n",
    "\t\t\tall_masks=[]\n",
    "\t\t\tall_labels=[]\n",
    "\t\t\tall_transforms=[]\n",
    "\n",
    "\t\t\tfor sentence in sentences:\n",
    "\t\t\t\ttok_ids=[]\n",
    "\t\t\t\tinput_mask=[]\n",
    "\t\t\t\tlabels=[]\n",
    "\t\t\t\ttransform=[]\n",
    "\n",
    "\t\t\t\tall_toks=[]\n",
    "\t\t\t\tn=0\n",
    "\t\t\t\tfor idx, word in enumerate(sentence):\n",
    "\t\t\t\t\ttoks=tokenizer.tokenize(word)\n",
    "\t\t\t\t\tall_toks.append(toks)\n",
    "\t\t\t\t\tn+=len(toks)\n",
    "\n",
    "\t\t\t\tcur=0\n",
    "\t\t\t\tfor idx, word in enumerate(sentence):\n",
    "\t\t\t\t\ttoks=all_toks[idx]\n",
    "\t\t\t\t\tind=list(np.zeros(n))\n",
    "\t\t\t\t\tfor j in range(cur,cur+len(toks)):\n",
    "\t\t\t\t\t\tind[j]=1./len(toks)\n",
    "\t\t\t\t\tcur+=len(toks)\n",
    "\t\t\t\t\ttransform.append(ind)\n",
    "\n",
    "\t\t\t\t\ttok_ids.extend(tokenizer.convert_tokens_to_ids(toks))\n",
    "\n",
    "\t\t\t\t\tinput_mask.extend(np.ones(len(toks)))\n",
    "\t\t\t\t\tlabels.append(1)\n",
    "\n",
    "\t\t\t\tall_data.append(tok_ids)\n",
    "\t\t\t\tall_masks.append(input_mask)\n",
    "\t\t\t\tall_labels.append(labels)\n",
    "\t\t\t\tall_transforms.append(transform)\n",
    "\n",
    "\t\t\tlengths = np.array([len(l) for l in all_data])\n",
    "\n",
    "\t\t\t# Note sequence must be ordered from shortest to longest so current_batch will work\n",
    "\t\t\tordering = np.argsort(lengths)\n",
    "\t\t\t\n",
    "\t\t\tordered_data = [None for i in range(len(all_data))]\n",
    "\t\t\tordered_masks = [None for i in range(len(all_data))]\n",
    "\t\t\tordered_labels = [None for i in range(len(all_data))]\n",
    "\t\t\tordered_transforms = [None for i in range(len(all_data))]\n",
    "\t\t\t\n",
    "\n",
    "\t\t\tfor i, ind in enumerate(ordering):\n",
    "\t\t\t\tordered_data[i] = all_data[ind]\n",
    "\t\t\t\tordered_masks[i] = all_masks[ind]\n",
    "\t\t\t\tordered_labels[i] = all_labels[ind]\n",
    "\t\t\t\tordered_transforms[i] = all_transforms[ind]\n",
    "\n",
    "\t\t\tbatched_data=[]\n",
    "\t\t\tbatched_mask=[]\n",
    "\t\t\tbatched_labels=[]\n",
    "\t\t\tbatched_transforms=[]\n",
    "\n",
    "\t\t\ti=0\n",
    "\t\t\tcurrent_batch=max_batch\n",
    "\n",
    "\t\t\twhile i < len(ordered_data):\n",
    "\n",
    "\t\t\t\tbatch_data=ordered_data[i:i+current_batch]\n",
    "\t\t\t\tbatch_mask=ordered_masks[i:i+current_batch]\n",
    "\t\t\t\tbatch_labels=ordered_labels[i:i+current_batch]\n",
    "\t\t\t\tbatch_transforms=ordered_transforms[i:i+current_batch]\n",
    "\n",
    "\t\t\t\tmax_len = max([len(sent) for sent in batch_data])\n",
    "\t\t\t\tmax_label = max([len(label) for label in batch_labels])\n",
    "\n",
    "\t\t\t\tfor j in range(len(batch_data)):\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tblen=len(batch_data[j])\n",
    "\t\t\t\t\tblab=len(batch_labels[j])\n",
    "\n",
    "\t\t\t\t\tfor k in range(blen, max_len):\n",
    "\t\t\t\t\t\tbatch_data[j].append(0)\n",
    "\t\t\t\t\t\tbatch_mask[j].append(0)\n",
    "\t\t\t\t\t\tfor z in range(len(batch_transforms[j])):\n",
    "\t\t\t\t\t\t\tbatch_transforms[j][z].append(0)\n",
    "\n",
    "\t\t\t\t\tfor k in range(blab, max_label):\n",
    "\t\t\t\t\t\tbatch_labels[j].append(-100)\n",
    "\n",
    "\t\t\t\t\tfor k in range(len(batch_transforms[j]), max_label):\n",
    "\t\t\t\t\t\tbatch_transforms[j].append(np.zeros(max_len))\n",
    "\n",
    "\t\t\t\tbatched_data.append(torch.LongTensor(batch_data))\n",
    "\t\t\t\tbatched_mask.append(torch.FloatTensor(batch_mask))\n",
    "\t\t\t\tbatched_labels.append(torch.LongTensor(batch_labels))\n",
    "\t\t\t\tbatched_transforms.append(torch.FloatTensor(batch_transforms))\n",
    "\n",
    "\t\t\t\tbsize=torch.FloatTensor(batch_transforms).shape\n",
    "\t\t\t\t\n",
    "\t\t\t\ti+=current_batch\n",
    "\n",
    "\t\t\t\t# adjust batch size; sentences are ordered from shortest to longest so decrease as they get longer\n",
    "\t\t\t\tif max_len > 100:\n",
    "\t\t\t\t\tcurrent_batch=12\n",
    "\t\t\t\tif max_len > 200:\n",
    "\t\t\t\t\tcurrent_batch=6\n",
    "\n",
    "\t\t\treturn batched_data, batched_mask, batched_transforms, ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3230934d-ca8f-4052-a78c-084159d6cece",
   "metadata": {},
   "outputs": [],
   "source": [
    "\tdef get_berts(self, raw_sents):\n",
    "\t\tsents=convert_to_toks(raw_sents)\n",
    "\t\tbatch_size=32\n",
    "\t\tbatched_data, batched_mask, batched_transforms, ordering=self.get_batches(sents, batch_size, self.wp_tokenizer)\n",
    "\n",
    "\t\tordered_preds=[]\n",
    "\t\tfor b in range(len(batched_data)):\n",
    "\t\t\tsize=batched_transforms[b].shape\n",
    "\t\t\tb_size=size[0]\n",
    "\t\t\tberts=self.model.forward(batched_data[b], attention_mask=batched_mask[b], transforms=batched_transforms[b])\n",
    "\t\t\tberts=berts.detach()\n",
    "\t\t\tberts=berts.cpu()\n",
    "\t\t\tfor row in range(b_size):\n",
    "\t\t\t\tordered_preds.append([np.array(r) for r in berts[row]])\n",
    "\n",
    "\t\tpreds_in_order = [None for i in range(len(sents))]\n",
    "\n",
    "\n",
    "\t\tfor i, ind in enumerate(ordering):\n",
    "\t\t\tpreds_in_order[ind] = ordered_preds[i]\n",
    "\n",
    "\n",
    "\t\tbert_sents=[]\n",
    "\n",
    "\t\tfor idx, sentence in enumerate(sents):\n",
    "\t\t\tbert_sent=[]\n",
    "\n",
    "\t\t\tbert_sent.append((\"[CLS]\", preds_in_order[idx][0] ))\n",
    "\n",
    "\t\t\tfor t_idx in range(1, len(sentence)-1):\n",
    "\t\t\t\ttoken=sentence[t_idx]\n",
    "\t\t\t\t\n",
    "\t\t\t\tpred=preds_in_order[idx][t_idx]\n",
    "\t\t\t\tbert_sent.append((token, pred ))\n",
    "\n",
    "\t\t\tbert_sent.append((\"[SEP]\", preds_in_order[idx][len(sentence)-1] ))\n",
    "\n",
    "\t\t\tbert_sents.append(bert_sent)\n",
    "\n",
    "\t\treturn bert_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86162bed-6c40-485c-84da-e24005edfc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_berts(self, raw_sents):\n",
    "        sents = convert_to_toks(raw_sents)\n",
    "        batch_size = 32\n",
    "        batched_data, batched_mask, batched_transforms, ordering = self.get_batches(sents, batch_size, self.wp_tokenizer)\n",
    "    \n",
    "        ordered_preds = []\n",
    "        # Corrected handling of outputs and attentions\n",
    "        for b in range(len(batched_data)):\n",
    "            size = batched_transforms[b].shape\n",
    "            b_size = size[0]\n",
    "            out, attentions = self.model.forward(batched_data[b], attention_mask=batched_mask[b], transforms=batched_transforms[b])  # This now correctly captures the tuple's items\n",
    "            out = out.detach()  # Detach only the output tensor\n",
    "            out = out.cpu()\n",
    "            for row in range(b_size):\n",
    "                ordered_preds.append([np.array(r) for r in out[row]])\n",
    "    \n",
    "        preds_in_order = [None for i in range(len(sents))]\n",
    "    \n",
    "        for i, ind in enumerate(ordering):\n",
    "            preds_in_order[ind] = ordered_preds[i]\n",
    "    \n",
    "        bert_sents = []\n",
    "    \n",
    "        for idx, sentence in enumerate(sents):\n",
    "            bert_sent = []\n",
    "    \n",
    "            bert_sent.append((\"[CLS]\", preds_in_order[idx][0]))\n",
    "    \n",
    "            for t_idx in range(1, len(sentence)-1):\n",
    "                token = sentence[t_idx]\n",
    "                pred = preds_in_order[idx][t_idx]\n",
    "                bert_sent.append((token, pred))\n",
    "    \n",
    "            bert_sent.append((\"[SEP]\", preds_in_order[idx][len(sentence)-1]))\n",
    "    \n",
    "            bert_sents.append(bert_sent)\n",
    "    \n",
    "        return bert_sents, attentions  \n",
    "\n",
    "\n",
    "\n",
    "class LatinTokenizer():\n",
    "\tdef __init__(self, encoder):\n",
    "\t\tself.vocab={}\n",
    "\t\tself.reverseVocab={}\n",
    "\t\tself.encoder=encoder\n",
    "\n",
    "\t\tself.vocab[\"[PAD]\"]=0\n",
    "\t\tself.vocab[\"[UNK]\"]=1\n",
    "\t\tself.vocab[\"[CLS]\"]=2\n",
    "\t\tself.vocab[\"[SEP]\"]=3\n",
    "\t\tself.vocab[\"[MASK]\"]=4\n",
    "\n",
    "\t\tfor key in self.encoder._subtoken_string_to_id:\n",
    "\t\t\tself.vocab[key]=self.encoder._subtoken_string_to_id[key]+5\n",
    "\t\t\tself.reverseVocab[self.encoder._subtoken_string_to_id[key]+5]=key\n",
    "\n",
    "\n",
    "\tdef convert_tokens_to_ids(self, tokens):\n",
    "\t\twp_tokens=[]\n",
    "\t\tfor token in tokens:\n",
    "\t\t\tif token == \"[PAD]\":\n",
    "\t\t\t\twp_tokens.append(0)\n",
    "\t\t\telif token == \"[UNK]\":\n",
    "\t\t\t\twp_tokens.append(1)\n",
    "\t\t\telif token == \"[CLS]\":\n",
    "\t\t\t\twp_tokens.append(2)\n",
    "\t\t\telif token == \"[SEP]\":\n",
    "\t\t\t\twp_tokens.append(3)\n",
    "\t\t\telif token == \"[MASK]\":\n",
    "\t\t\t\twp_tokens.append(4)\n",
    "\n",
    "\t\t\telse:\n",
    "\t\t\t\twp_tokens.append(self.vocab[token])\n",
    "\n",
    "\t\treturn wp_tokens\n",
    "\n",
    "\tdef tokenize(self, text):\n",
    "\t\ttokens=text.split(\" \")\n",
    "\t\twp_tokens=[]\n",
    "\t\tfor token in tokens:\n",
    "\n",
    "\t\t\tif token in {\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"}:\n",
    "\t\t\t\twp_tokens.append(token)\n",
    "\t\t\telse:\n",
    "\n",
    "\t\t\t\twp_toks=self.encoder.encode(token)\n",
    "\n",
    "\t\t\t\tfor wp in wp_toks:\n",
    "\t\t\t\t\twp_tokens.append(self.reverseVocab[wp+5])\n",
    "\n",
    "\t\treturn wp_tokens\n",
    "\n",
    "def convert_to_toks(sents):\n",
    "\n",
    "\tsent_tokenizer = SentenceTokenizer()\n",
    "\tword_tokenizer = WordTokenizer()\n",
    "\n",
    "\tall_sents=[]\n",
    "\n",
    "\tfor data in sents:\n",
    "\t\ttext=data.lower()\n",
    "\n",
    "\t\tsents=sent_tokenizer.tokenize(text)\n",
    "\t\tfor sent in sents:\n",
    "\t\t\ttokens=word_tokenizer.tokenize(sent)\n",
    "\t\t\tfilt_toks=[]\n",
    "\t\t\tfilt_toks.append(\"[CLS]\")\n",
    "\t\t\tfor tok in tokens:\n",
    "\t\t\t\tif tok != \"\":\n",
    "\t\t\t\t\tfilt_toks.append(tok)\n",
    "\t\t\tfilt_toks.append(\"[SEP]\")\n",
    "\n",
    "\t\t\tall_sents.append(filt_toks)\n",
    "\n",
    "\treturn all_sents\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BertLatin(nn.Module):\n",
    "\n",
    "\tdef __init__(self, bertPath=None):\n",
    "\t\tsuper(BertLatin, self).__init__()\n",
    "\n",
    "\t\tself.bert = BertModel.from_pretrained(bertPath)\n",
    "\t\tself.bert.eval()\n",
    "\t\t\n",
    "\tdef forward(self, input_ids, token_type_ids=None, attention_mask=None, transforms=None):\n",
    "\n",
    "\t\tinput_ids = input_ids.to(device)\n",
    "\t\tattention_mask = attention_mask.to(device)\n",
    "\t\ttransforms = transforms.to(device)\n",
    "\t\t\n",
    "\t\toutputs = self.bert.forward(input_ids, token_type_ids=None, attention_mask=attention_mask)\n",
    "\t\t# 'outputs' from BERT is a transformers.modeling_outputs.BaseModelOutputWithCrossAttentions object\n",
    "\t\tsequence_outputs = outputs[0]\n",
    "\t\tpooled_outputs = outputs[1]\n",
    "\t\t\n",
    "\t\tall_layers=sequence_outputs\n",
    "\t\tout=torch.matmul(transforms,all_layers)\n",
    "\t\treturn out\n",
    "# python3 scripts/gen_berts.py --bertPath models/latin_bert/ --tokenizerPath models/subword_tokenizer_latin/latin.subword.encoder\n",
    "# if __name__ == \"__main__\":\n",
    "# \n",
    "# \tparser = argparse.ArgumentParser()\n",
    "# \tparser.add_argument('-b', '--bertPath', help='path to pre-trained BERT', required=True)\n",
    "# \tparser.add_argument('-t', '--tokenizerPath', help='path to Latin WordPiece tokenizer', required=True)\n",
    "# \t\n",
    "# \targs = vars(parser.parse_args())\n",
    "# \n",
    "# \tbertPath=args[\"bertPath\"]\n",
    "# \ttokenizerPath=args[\"tokenizerPath\"]\t\t\t\n",
    "# \n",
    "# \tbert=LatinBERT(tokenizerPath=tokenizerPath, bertPath=bertPath)\n",
    "# \n",
    "# \tsents=[\"arma virumque cano\", \"arma gravi numero violentaque bella parabam\"]\n",
    "# \t\n",
    "# \tbert_sents=bert.get_berts(sents)\n",
    "# \n",
    "# \tfor sent in bert_sents:\n",
    "# \t\tfor (token, bert) in sent:\n",
    "# \t\t\tprint(\"%s\\t%s\" % ( token, ' '.join([\"%.5f\" % x for x in bert])))\n",
    "# \t\tprint()\n",
    "\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3aa31007-a41b-4775-bcf3-67d71aebb90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = [\"Si quid est in me ingeni, iudices, quod sentio quam sit exiguum, aut si qua exercitatio dicendi in qua me non infitior mediocriter esse versatum, aut si huiusce rei ratio aliqua ab optimarum artium studiis ac disciplina profecta, a qua ego nullum confiteor aetatis meae tempus abhorruisse, earum rerum omnium vel in primis hic A. Licinius fructum a me repetere prope suo iure debet.\",\n",
    "         \"Ac ne quis a nobis hoc ita dici forte miretur, quod alia quaedam in hoc facultas sit ingeni, neque haec dicendi ratio aut disciplina, ne nos quidem huic uni studio penitus umquam dediti fuimus.\"]\n",
    "bert = LatinBERT(tokenizerPath=\"/slipstream_old/home/juliusherzog/latinbert/models/subword_tokenizer_latin/latin.subword.encoder\",\n",
    "                 bertPath=\"/slipstream_old/home/juliusherzog/latinbert/models/latin_bert/\")\n",
    "bert_sents, attentions = bert.get_berts(sents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "116938b3-45c5-4dbb-bcb6-d12587d75981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Obtaining dependency information for ipywidgets from https://files.pythonhosted.org/packages/70/1a/7edeedb1c089d63ccd8bd5c0612334774e90cf9337de9fe6c82d90081791/ipywidgets-8.1.2-py3-none-any.whl.metadata\n",
      "  Downloading ipywidgets-8.1.2-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in /slipstream_old/home/juliusherzog/miniconda3/envs/latinbert/lib/python3.8/site-packages (from ipywidgets) (0.1.4)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /slipstream_old/home/juliusherzog/miniconda3/envs/latinbert/lib/python3.8/site-packages (from ipywidgets) (8.12.3)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /slipstream_old/home/juliusherzog/miniconda3/envs/latinbert/lib/python3.8/site-packages (from ipywidgets) (5.12.0)\n",
      "Collecting widgetsnbextension~=4.0.10 (from ipywidgets)\n",
      "  Obtaining dependency information for widgetsnbextension~=4.0.10 from https://files.pythonhosted.org/packages/99/bc/82a8c3985209ca7c0a61b383c80e015fd92e74f8ba0ec1af98f9d6ca8dce/widgetsnbextension-4.0.10-py3-none-any.whl.metadata\n",
      "  Downloading widgetsnbextension-4.0.10-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab-widgets~=3.0.10 (from ipywidgets)\n",
      "  Obtaining dependency information for jupyterlab-widgets~=3.0.10 from https://files.pythonhosted.org/packages/24/da/db1cb0387a7e4086780aff137987ee924e953d7f91b2a870f994b9b1eeb8/jupyterlab_widgets-3.0.10-py3-none-any.whl.metadata\n",
      "  Downloading jupyterlab_widgets-3.0.10-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: backcall in /slipstream_old/home/juliusherzog/miniconda3/envs/latinbert/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in /slipstream_old/home/juliusherzog/.local/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /slipstream_old/home/juliusherzog/miniconda3/envs/latinbert/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /slipstream_old/home/juliusherzog/miniconda3/envs/latinbert/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in /slipstream_old/home/juliusherzog/miniconda3/envs/latinbert/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /slipstream_old/home/juliusherzog/miniconda3/envs/latinbert/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.39)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /slipstream_old/home/juliusherzog/miniconda3/envs/latinbert/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (2.16.1)\n",
      "Requirement already satisfied: stack-data in /slipstream_old/home/juliusherzog/miniconda3/envs/latinbert/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing-extensions in /slipstream_old/home/juliusherzog/miniconda3/envs/latinbert/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.4.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /slipstream_old/home/juliusherzog/miniconda3/envs/latinbert/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /slipstream_old/home/juliusherzog/miniconda3/envs/latinbert/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /slipstream_old/home/juliusherzog/miniconda3/envs/latinbert/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /slipstream_old/home/juliusherzog/miniconda3/envs/latinbert/lib/python3.8/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.8)\n",
      "Requirement already satisfied: executing>=1.2.0 in /slipstream_old/home/juliusherzog/miniconda3/envs/latinbert/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /slipstream_old/home/juliusherzog/miniconda3/envs/latinbert/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.0)\n",
      "Requirement already satisfied: pure-eval in /slipstream_old/home/juliusherzog/miniconda3/envs/latinbert/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /slipstream_old/home/juliusherzog/.local/lib/python3.8/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Downloading ipywidgets-8.1.2-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading jupyterlab_widgets-3.0.10-py3-none-any.whl (215 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.0/215.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading widgetsnbextension-4.0.10-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.2 jupyterlab-widgets-3.0.10 widgetsnbextension-4.0.10\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c09fea39-d53a-4c83-98b9-70b4f985adfd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Initialize the tokenizer\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mBertTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbert-base-multilingual-cased\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Example, adjust based on your specific model\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Tokenize input text\u001b[39;00m\n\u001b[1;32m      8\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(sents, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/latinbert/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1425\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1363\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1364\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_pretrained\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1366\u001b[0m \u001b[38;5;124;03m    Instantiate a :class:`~transformers.tokenization_utils_base.PreTrainedTokenizerBase` (or a derived class) from\u001b[39;00m\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;124;03m    a predefined tokenizer.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1423\u001b[0m \n\u001b[1;32m   1424\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/latinbert/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1498\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m             resolved_vocab_files[file_id] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1498\u001b[0m             resolved_vocab_files[file_id] \u001b[38;5;241m=\u001b[39m \u001b[43mcached_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1499\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1500\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m                \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m                \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m                \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[1;32m   1507\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pretrained_model_name_or_path \u001b[38;5;129;01min\u001b[39;00m s3_models:\n",
      "File \u001b[0;32m~/miniconda3/envs/latinbert/lib/python3.8/site-packages/transformers/file_utils.py:683\u001b[0m, in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, local_files_only)\u001b[0m\n\u001b[1;32m    679\u001b[0m     cache_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(cache_dir)\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_remote_url(url_or_filename):\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;66;03m# URL, so get it from the cache (downloading if necessary)\u001b[39;00m\n\u001b[0;32m--> 683\u001b[0m     output_path \u001b[38;5;241m=\u001b[39m \u001b[43mget_from_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_or_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(url_or_filename):\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;66;03m# File, and it exists.\u001b[39;00m\n\u001b[1;32m    694\u001b[0m     output_path \u001b[38;5;241m=\u001b[39m url_or_filename\n",
      "File \u001b[0;32m~/miniconda3/envs/latinbert/lib/python3.8/site-packages/transformers/file_utils.py:869\u001b[0m, in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only)\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m temp_file_manager() \u001b[38;5;28;01mas\u001b[39;00m temp_file:\n\u001b[1;32m    867\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m not found in cache or force_download set to True, downloading to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, temp_file\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m--> 869\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    871\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstoring \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m in cache at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, cache_path)\n\u001b[1;32m    872\u001b[0m os\u001b[38;5;241m.\u001b[39mreplace(temp_file\u001b[38;5;241m.\u001b[39mname, cache_path)\n",
      "File \u001b[0;32m~/miniconda3/envs/latinbert/lib/python3.8/site-packages/transformers/file_utils.py:754\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, user_agent)\u001b[0m\n\u001b[1;32m    752\u001b[0m content_length \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    753\u001b[0m total \u001b[38;5;241m=\u001b[39m resume_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mint\u001b[39m(content_length) \u001b[38;5;28;01mif\u001b[39;00m content_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 754\u001b[0m progress \u001b[38;5;241m=\u001b[39m \u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[43munit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43munit_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDownloading\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    760\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlogging\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_verbosity\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlogging\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNOTSET\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    761\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m):\n\u001b[1;32m    763\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/latinbert/lib/python3.8/site-packages/tqdm/notebook.py:218\u001b[0m, in \u001b[0;36mtqdm_notebook.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m unit_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit_scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit_scale \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    217\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;241m*\u001b[39m unit_scale \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal\n\u001b[0;32m--> 218\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_printer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mncols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplay\n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m# Print initial bar state\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/latinbert/lib/python3.8/site-packages/tqdm/notebook.py:96\u001b[0m, in \u001b[0;36mtqdm_notebook.status_printer\u001b[0;34m(_, total, desc, ncols)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Fallback to text bar if there's no total\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# DEPRECATED: replaced with an 'info' style bar\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# if not total:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     93\u001b[0m \n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Prepare IPython progress bar\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m IProgress \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# #187 #451 #558 #872\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIProgress not found. Please update jupyter and ipywidgets.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m See https://ipywidgets.readthedocs.io/en/stable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/user_install.html\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total:\n\u001b[1;32m    101\u001b[0m     pbar \u001b[38;5;241m=\u001b[39m IProgress(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39mtotal)\n",
      "\u001b[0;31mImportError\u001b[0m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "from bertviz import model_view\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")  # Example, adjust based on your specific model\n",
    "\n",
    "# Tokenize input text\n",
    "inputs = tokenizer(sents, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# Assuming `attentions` is obtained and formatted correctly\n",
    "# Visualize the attention\n",
    "# Note: This is a simplified example; you will need to adapt it based on the actual structure of `attentions`\n",
    "model_view(attentions, inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
